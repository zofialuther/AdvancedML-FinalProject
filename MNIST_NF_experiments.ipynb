{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Flows for MNIST Embeddings\n",
    "### Adapted from: Jary Pomponi & Simone Scardapane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import rnvp\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 20 # The dimension of the embeddings\n",
    "FLOW_N = 32 # Number of affine coupling layers\n",
    "RNVP_TOPOLOGY = [200] # Size of the hidden layers in each coupling layer\n",
    "AE_EPOCHS = 10 # Epochs for training the autoencoder\n",
    "NF_EPOCHS = 20 # Epochs for training the normalizing flow\n",
    "SEED = 0 # Seed of the random number generator\n",
    "BATCH_SIZE = 100 # Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple autoencoder for images. \n",
    "    self.linear1 generates the intermediate embeddings that we use for the normalizing flow.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoding layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, stride=2, kernel_size=3, bias=False, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, stride=2, kernel_size=3, bias=False, padding=1)\n",
    "        self.linear1 = nn.Linear(in_features=3136, out_features=EMBEDDING_DIM)\n",
    "        \n",
    "        # Decoding layers\n",
    "        self.linear2 = nn.Linear(in_features=EMBEDDING_DIM, out_features=3136)\n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels=64, out_channels=32, stride=2, kernel_size=3, padding=1, output_padding=1)\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels=32, out_channels=1, stride=2, kernel_size=3, padding=1, output_padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        emb = self.encoder(x)\n",
    "        _x = self.decoder(emb)\n",
    "        \n",
    "        return _x, emb\n",
    "    \n",
    "    def decoder(self, emb):\n",
    "\n",
    "        _x = torch.relu(self.linear2(emb))\n",
    "        _x = _x.view(-1, 64, 7, 7)\n",
    "        _x = torch.relu(self.convt1(_x))\n",
    "        _x = self.convt2(_x)\n",
    "        \n",
    "        return _x\n",
    "    \n",
    "    def encoder(self, x):\n",
    "        _x = torch.relu(self.conv1(x))\n",
    "        _x = torch.relu(self.conv2(_x))\n",
    "        sh = _x.shape\n",
    "\n",
    "        _x = torch.relu(torch.flatten(_x, 1))\n",
    "        \n",
    "        emb = self.linear1(_x)\n",
    "        \n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder training on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_set = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "train_loader = torch.utils.data.DataLoader(train_set, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "Epoch #2\n",
      "Epoch #3\n",
      "Epoch #4\n",
      "Epoch #5\n",
      "Epoch #6\n",
      "Epoch #7\n",
      "Epoch #8\n",
      "Epoch #9\n",
      "Epoch #10\n"
     ]
    }
   ],
   "source": [
    "# We use a binary cross-entropy loss for the reconstruction error\n",
    "loss_f = nn.BCELoss()\n",
    "\n",
    "# Build the autoencoder\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder = autoencoder.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(autoencoder.parameters()),\n",
    "                             lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "for i in range(AE_EPOCHS):\n",
    "    print('Epoch #{}'.format(i+1))\n",
    "\n",
    "    losses = []\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "\n",
    "        x, _ = data\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Run the autoencoder\n",
    "        _x, emb = autoencoder(x)\n",
    "        _x = torch.sigmoid(_x)\n",
    "\n",
    "        # Compute loss\n",
    "        rec_loss = loss_f(_x, x)\n",
    "\n",
    "        autoencoder.zero_grad()\n",
    "        rec_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new dataset containing the embeddings and the associated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace the original x with the corresponding embedding from the trained autoencoder\n",
    "\n",
    "embs = []\n",
    "for batch_idx, data in enumerate(train_loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x, y = data\n",
    "\n",
    "        x = x.to(device)\n",
    "\n",
    "        _, emb = autoencoder(x)\n",
    "        for i in range(len(emb)):\n",
    "            embs.append((emb[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_loader = torch.utils.data.DataLoader(embs, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Flow training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "Epoch #2\n",
      "Epoch #3\n",
      "Epoch #4\n",
      "Epoch #5\n",
      "Epoch #6\n",
      "Epoch #7\n",
      "Epoch #8\n",
      "Epoch #9\n",
      "Epoch #10\n",
      "Epoch #11\n",
      "Epoch #12\n",
      "Epoch #13\n",
      "Epoch #14\n",
      "Epoch #15\n",
      "Epoch #16\n",
      "Epoch #17\n",
      "Epoch #18\n",
      "Epoch #19\n"
     ]
    }
   ],
   "source": [
    "# See the file realmvp.py for the full definition\n",
    "nf_model = rnvp.LinearRNVP(input_dim=EMBEDDING_DIM, coupling_topology=RNVP_TOPOLOGY, flow_n=FLOW_N, batch_norm=True,\n",
    "                      mask_type='odds', conditioning_size=10, use_permutation=True, single_function=True)\n",
    "nf_model = nf_model.to(device)\n",
    "\n",
    "optimizer1 = torch.optim.Adam(itertools.chain(nf_model.parameters()), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "nf_model.train()\n",
    "for i in range(NF_EPOCHS):\n",
    "    print('Epoch #{}'.format(i+1))\n",
    "\n",
    "    losses = []\n",
    "    for batch_idx, data in enumerate(embs_loader):\n",
    "\n",
    "        emb, y = data\n",
    "        emb = emb.to(device)\n",
    "        y = y.to(device)\n",
    "        y = torch.nn.functional.one_hot(y, 10).to(device).float()\n",
    "        \n",
    "        # Get the inverse transformation and the corresponding log determinant of the Jacobian\n",
    "        u, log_det = nf_model.forward(emb, y=y) \n",
    "\n",
    "        # Train via maximum likelihood\n",
    "        prior_logprob = nf_model.logprob(u)\n",
    "        log_prob = -torch.mean(prior_logprob.sum(1) + log_det)\n",
    "\n",
    "        nf_model.zero_grad()\n",
    "\n",
    "        log_prob.backward()\n",
    "\n",
    "        optimizer1.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the NF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_n = 10\n",
    "f, axs = plt.subplots(nrows=10, ncols=sample_n, figsize=(20, 20))\n",
    "\n",
    "nf_model.eval()\n",
    "with torch.no_grad():\n",
    "    for j in range(10):\n",
    "\n",
    "        y = torch.nn.functional.one_hot(torch.tensor([j]*sample_n), 10).to(device).float()\n",
    "        emb, d = nf_model.sample(sample_n, y=y, return_logdet=True)\n",
    "        z = autoencoder.decoder(emb)\n",
    "\n",
    "        d_sorted = d.sort(0)[1].flip(0)\n",
    "        z = z[d_sorted]\n",
    "        z = torch.sigmoid(z).cpu()\n",
    "        \n",
    "        for i in range(sample_n):\n",
    "            axs[j][i].imshow(z[i].reshape(28, 28), cmap='gray')\n",
    "\n",
    "for ax in axs:\n",
    "    for a in ax:\n",
    "        a.set_xticklabels([])\n",
    "        a.set_yticklabels([])\n",
    "        a.set_aspect('equal')\n",
    "\n",
    "f.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the file realmvp.py for the full definition\n",
    "nf_model = rnvp.LinearRNVPUniform(input_dim=EMBEDDING_DIM, coupling_topology=RNVP_TOPOLOGY, flow_n=FLOW_N, batch_norm=True,\n",
    "                      mask_type='odds', conditioning_size=10, use_permutation=True, single_function=True)\n",
    "nf_model = nf_model.to(device)\n",
    "\n",
    "optimizer1 = torch.optim.Adam(itertools.chain(nf_model.parameters()), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "nf_model.train()\n",
    "for i in range(NF_EPOCHS):\n",
    "    print('Epoch #{}'.format(i+1))\n",
    "\n",
    "    losses = []\n",
    "    for batch_idx, data in enumerate(embs_loader):\n",
    "\n",
    "        emb, y = data\n",
    "        emb = emb.to(device)\n",
    "        y = y.to(device)\n",
    "        y = torch.nn.functional.one_hot(y, 10).to(device).float()\n",
    "        \n",
    "        # Get the inverse transformation and the corresponding log determinant of the Jacobian\n",
    "        u, log_det = nf_model.forward(emb, y=y) \n",
    "\n",
    "        # print(\"prior_logprob shape:\", prior_logprob.shape)\n",
    "        # print(\"log_det shape:\", log_det.shape)\n",
    "\n",
    "        # Train via maximum likelihood\n",
    "        prior_logprob = nf_model.logprob(u)\n",
    "        log_prob = -torch.mean(prior_logprob.sum(0) + log_det)\n",
    "\n",
    "        nf_model.zero_grad()\n",
    "\n",
    "        log_prob.backward()\n",
    "\n",
    "        optimizer1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_n = 10\n",
    "f, axs = plt.subplots(nrows=10, ncols=sample_n, figsize=(20, 20))\n",
    "\n",
    "nf_model.eval()\n",
    "with torch.no_grad():\n",
    "    for j in range(10):\n",
    "\n",
    "        y = torch.nn.functional.one_hot(torch.tensor([j]*sample_n), 10).to(device).float()\n",
    "        emb, d = nf_model.sample(sample_n, y=y, return_logdet=True)\n",
    "        z = autoencoder.decoder(emb)\n",
    "\n",
    "        d_sorted = d.sort(0)[1].flip(0)\n",
    "        z = z[d_sorted]\n",
    "        z = torch.sigmoid(z).cpu()\n",
    "        \n",
    "        for i in range(sample_n):\n",
    "            axs[j][i].imshow(z[i].reshape(28, 28), cmap='gray')\n",
    "\n",
    "for ax in axs:\n",
    "    for a in ax:\n",
    "        a.set_xticklabels([])\n",
    "        a.set_yticklabels([])\n",
    "        a.set_aspect('equal')\n",
    "\n",
    "f.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
